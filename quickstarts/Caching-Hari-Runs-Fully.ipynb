{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "navG4j6eqc_o"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0lpR41ozqBFp"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9g4WhqtqiV-"
      },
      "source": [
        "# Gemini API: Context Caching Quickstart\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Caching.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCC-LFNtqogI"
      },
      "source": [
        "This notebook introduces context caching with the Gemini API and provides examples of interacting with the Apollo 11 transcript using the Python SDK. For a more comprehensive look, check out [the caching guide](https://ai.google.dev/gemini-api/docs/caching?lang=python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yRLvyPhrXSf"
      },
      "outputs": [],
      "source": [
        "!pip install -qU 'google-generativeai>=0.7.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4xdJSvLerazn"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.generativeai import caching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrJYxEpOrc3d"
      },
      "source": [
        "## Configure your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EgO56yWoriI0"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=userdata.get(\"GOOGLE_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T-ZorTqrsoz"
      },
      "source": [
        "## Upload a file\n",
        "\n",
        "A common pattern with the Gemini API is to ask a number of questions of the same document. Context caching is designed to assist with this case, and can be more efficient by avoiding the need to pass the same tokens through the model for each new request.\n",
        "\n",
        "Start by uploading a large file with the [File API](https://github.com/google-gemini/cookbook/blob/main/quickstarts/File_API.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sa-r2s_ltBXy",
        "outputId": "0be20530-2067-4429-f705-03633f1fef75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INTRODUCTION\n",
            "\n",
            "This is the transcription of the Technical Air-to-Ground Voice Transmission (GOSS NET 1) from the Apollo 11 mission.\n",
            "\n",
            "Communicators in the text may be identified according to the following list.\n",
            "\n",
            "Spacecraft:\n",
            "CDR\tCommander\tNeil A. Armstrong\n",
            "CMP\tCommand module pilot   \tMichael Collins\n",
            "LMP\tLunar module pilot\tEdwin E. ALdrin, Jr.\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
        "!head a11.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To reduce the cost of experiment. I want to use only 1/10 of the file.\n",
        "\n",
        "with open('a11.txt', 'r') as file:\n",
        "  text = file.read()\n",
        "tenth_of_text = text[:len(text)//10]\n",
        "\n",
        "with open('a12.txt', 'w') as file:\n",
        "  file.write(tenth_of_text)\n"
      ],
      "metadata": {
        "id": "DPX7ch9K0KPR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b3bU9AvcvZ_x"
      },
      "outputs": [],
      "source": [
        "document = genai.upload_file(path=\"a12.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4LP7unAvmce"
      },
      "source": [
        "## Cache the prompt\n",
        "\n",
        "Next create a [`CachedContent`](https://ai.google.dev/api/python/google/generativeai/protos/CachedContent) object specifying the prompt you want to use, including the file and other fields you wish to cache. In this example the `system_instruction` has been set, and the document was provided in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "55V-QkaWv4tb",
        "outputId": "3f47a720-25d6-48fd-b639-1c4453718fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CachedContent(\n",
              "    name='cachedContents/c1tp9dntx6cs',\n",
              "    model='models/gemini-1.5-flash-001',\n",
              "    display_name='',\n",
              "    usage_metadata={\n",
              "        'total_token_count': 33731,\n",
              "    },\n",
              "    create_time=2024-08-04 09:58:24.234303+00:00,\n",
              "    update_time=2024-08-04 09:58:24.234303+00:00,\n",
              "    expire_time=2024-08-04 10:58:23.465918+00:00\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Note that caching requires a frozen model, e.g. one with a `-001` version suffix.\n",
        "model_name = \"gemini-1.5-flash-001\"\n",
        "\n",
        "apollo_cache = caching.CachedContent.create(\n",
        "    model=model_name,\n",
        "    system_instruction=\"You are an expert at analyzing transcripts.\",\n",
        "    contents=[document],\n",
        ")\n",
        "\n",
        "apollo_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j5yzay5xyPC"
      },
      "source": [
        "## Manage the cache expiry\n",
        "\n",
        "Once you have a `CachedContent` object, you can update the expiry time to keep it alive while you need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cUJT2ESUyTGb",
        "outputId": "df910c76-f650-40ac-e358-8abd6d72eb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CachedContent(\n",
              "    name='cachedContents/c1tp9dntx6cs',\n",
              "    model='models/gemini-1.5-flash-001',\n",
              "    display_name='',\n",
              "    usage_metadata={\n",
              "        'total_token_count': 33731,\n",
              "    },\n",
              "    create_time=2024-08-04 09:58:24.234303+00:00,\n",
              "    update_time=2024-08-04 09:58:36.094840+00:00,\n",
              "    expire_time=2024-08-04 11:58:36.074026+00:00\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "apollo_cache.update(ttl=datetime.timedelta(hours=2))\n",
        "apollo_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_PWabuayrf-"
      },
      "source": [
        "## Use the cache for generation\n",
        "\n",
        "As the `CachedContent` object refers to a specific model and parameters, you must create a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel) using [`from_cached_content`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#from_cached_content). Then, generate content as you would with a directly instantiated model object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EG8VNpuIzGwT",
        "outputId": "551e073d-cd9e-47ed-94ae-ea838f25f8d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A lighthearted moment occurs on page 11 of the transcript, around 01:42:45:\n",
            "\n",
            "**CMP:**  \"Houston, Apollo 11. We're standing by for a GO for sequence logic ON.\"\n",
            "**CC:** \"Apollo 11, this is Houston. Go ahead and we'll watch you on TM.\"\n",
            "**CDR:** \"Okay. Sequence logic, two of them. Sequence logic 1 and 2 coming up and ON.\" \n",
            "\n",
            "This brief exchange shows the astronauts' playful use of language (\"coming up\" and \"ON\") in contrast to the serious nature of their mission. It reflects the astronauts' ability to maintain a sense of humor even in the midst of a stressful situation. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "apollo_model = genai.GenerativeModel.from_cached_content(cached_content=apollo_cache)\n",
        "\n",
        "response = apollo_model.generate_content(\"Find a lighthearted moment from this transcript\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzixLQhC3AO2"
      },
      "source": [
        "You can inspect token usage through `usage_metadata`. Note that the cached prompt tokens are included in `prompt_token_count`, but excluded from the `total_token_count`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MLFd8DFZ29lC",
        "outputId": "e0e7c127-fa80-4d4e-9884-612baadb4db2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "prompt_token_count: 33740\n",
              "candidates_token_count: 154\n",
              "total_token_count: 33894\n",
              "cached_content_token_count: 33731"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "response.usage_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-24t14t302N"
      },
      "source": [
        "You can ask new questions of the model, and the cache is reused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pZngmGj13k9O",
        "outputId": "769b2d32-3e1e-4a33-f022-8339f9df3e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "StopCandidateException",
          "evalue": "finish_reason: SAFETY\nindex: 0\nsafety_ratings {\n  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HATE_SPEECH\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HARASSMENT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_DANGEROUS_CONTENT\n  probability: MEDIUM\n}\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopCandidateException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6396bf5b4936>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapollo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Give me a quote from the most important part of the transcript.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36msend_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    586\u001b[0m         )\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_automatic_function_calling\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtools_lib\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36m_check_response\u001b[0;34m(self, response, stream)\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0mprotos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCandidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFinishReason\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_TOKENS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             ):\n\u001b[0;32m--> 616\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopCandidateException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_function_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprotos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionCall\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopCandidateException\u001b[0m: finish_reason: SAFETY\nindex: 0\nsafety_ratings {\n  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HATE_SPEECH\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_HARASSMENT\n  probability: NEGLIGIBLE\n}\nsafety_ratings {\n  category: HARM_CATEGORY_DANGEROUS_CONTENT\n  probability: MEDIUM\n}\n"
          ]
        }
      ],
      "source": [
        "chat = apollo_model.start_chat()\n",
        "response = chat.send_message(\"Give me a quote from the most important part of the transcript.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = apollo_model.start_chat()\n",
        "response = chat.send_message(\"Give me a quote from the most important part of the transcript.\",\n",
        "safety_settings={\n",
        "        'HATE': 'BLOCK_NONE',\n",
        "        'HARASSMENT': 'BLOCK_NONE',\n",
        "        'SEXUAL' : 'BLOCK_NONE',\n",
        "        'DANGEROUS' : 'BLOCK_NONE'\n",
        "    }\n",
        "                             )\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "r3sgtQq11zRb",
        "outputId": "62c238d8-c748-4a61-feb1-ca2ab1984718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most important part of the transcript is the section detailing the Translunar Injection (TLI) burn, which marked the beginning of Apollo 11's journey to the Moon. \n",
            "\n",
            "Here's a quote from that section: \n",
            "\n",
            "**\"00 02 26 38 CC\n",
            "Apollo 11, this is Houston. You are GO for TLI. Over.\"**\n",
            "\n",
            "This quote signifies the moment Mission Control gave the crew the green light to initiate the TLI burn, propelling them on their historic journey to the lunar surface. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GhGTutW65u7h",
        "outputId": "a8e52802-cf7b-4f08-fd52-9896d73e8b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Mission Control gave the \"GO\" for TLI, the transcript records the following:\n",
            "\n",
            "* **Confirmation from the crew:**  \n",
            "   \"00 02 26 45 CMP\n",
            "   Apollo 11. Thank you.\"\n",
            "\n",
            "* **Confirmation from Mission Control:** \n",
            "   \"00 02 26 48 CC\n",
            "   Roger. Out.\"\n",
            "\n",
            "* **Communication about ARIA coverage:**\n",
            "    \"00 02 30 11 CC\n",
            "   Apollo 11, this is Houston. Over.\"\n",
            "    \"00 02 30 16 CDR\n",
            "   Houston, 11.\"\n",
            "    \"00 02 30 18 CC\n",
            "   Roger. We'll be coming within range of the ARIA aircraft coverage, here, in about 1 minute. They're going to try uplinking both on S-band and on VHF this time. So if you turn your - make sure your S-band volume is turned up, we'd appreciate it. And we believe that we'll have continuous coverage from now on through the TLI burn. Over.\"\n",
            "    \"00 02 30 42 CDR\n",
            "   Very good.\" \n",
            "\n",
            "* **The start of the TLI burn:**\n",
            "    \"00 02 44 19 CMP\n",
            "   Ignition.\" \n",
            "    \"00 02 44 27 CC\n",
            "   We confirm ignition, and the thrust is GO.\"\n",
            "\n",
            "* **Updates during the TLI burn:**\n",
            "    \"00 02 45 14 CC\n",
            "   Apollo 11, this is Houston at 1 minute. Trajectory and guidance look good, and the stage is good. Over.\"\n",
            "    \"00 02 45 23 CDR\n",
            "   Apollo 11. Roger.\" \n",
            "    \"00 02 46 26 CC\n",
            "   Apollo 11, this is Houston. Thrust is good. Everything's still looking good.\"\n",
            "    \"00 02 46 32 CDR\n",
            "   Roger.\"\n",
            "    \"00 02 47 54 CC\n",
            "   Apollo 11, this is Houston. Around 3-1/2 minutes. You're still looking good. Your predicted cut-off is right on the nominal.\"\n",
            "    \"00 02 48 04 CDR\n",
            "   Roger. Apollo 11 is GO.\"\n",
            "    \"00 02 49 18 CC\n",
            "   Apollo 11, this is Houston. You are GO at 5 minutes.\"\n",
            "    \"00 02 49 22 CDR\n",
            "   Roger. We're GO.\"\n",
            "\n",
            "* **The end of the TLI burn:**\n",
            "    \"00 02 50 36 CC\n",
            "   Apollo 11, this is Houston. We show cut-off and we copy the numbers in NOUN 62.\"\n",
            "    \"00 02 50 54 CC\n",
            "   Apollo 11, Houston. Do you read?\"\n",
            "    \"00 02 51 28 CC\n",
            "   Apollo 11, this is Houston. Do you read? Over.\"\n",
            "    \"00 02 51 21 LMP\n",
            "   Roger, Houston. Apollo 11. We're reading a VI of 35579 and the EMS was plus 3.3. Over.\"\n",
            "    \"00 02 51 31 CC\n",
            "   Roger. Plus 3.3 on the EMS. And we copy the VI.\"\n",
            "\n",
            "* **Reactions to the successful burn:** \n",
            "    \"00 02 53 03 CDR\n",
            "   Hey, Houston, Apollo 11. That Saturn gave us a magnificent ride.\"\n",
            "    \"00 02 53 0,7 CC\n",
            "   Roger, 11. We'll pass that on. And, it certainly looks like you are well on your way now.\"\n",
            "    \"00 02 53 30 CDR\n",
            "   We have no complaints with any of the three stages on that ride. It was beautiful.\" \n",
            "    \"00 02 53 38 CC\n",
            "   Roger. We copy. No transients at staging of any significance. Over.\"\n",
            "    \"00 02 53 44 CDR\n",
            "   That's right. It was all - all a good ride.\"\n",
            "    \"00 02 53 47 CC\n",
            "   Houston. Roger. Out.\"\n",
            "\n",
            "These exchanges demonstrate the careful coordination and communication between the crew and Mission Control during a critical phase of the Apollo 11 mission. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"What was recounted after that?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SB5Ywx2D6cOn",
        "outputId": "e652c7b0-7121-4b85-892e-0d169198f3dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "prompt_token_count: 33872\n",
              "candidates_token_count: 1033\n",
              "total_token_count: 34905\n",
              "cached_content_token_count: 33731"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "response.usage_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlU43ByS7Kt2"
      },
      "source": [
        "## Counting tokens\n",
        "\n",
        "The `GenerativeModel` object can be used to count the tokens of a request in the same manner as a direct, uncached, model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1zzHVCAK7ahI",
        "outputId": "046c2ada-7e48-4711-ae13-50574853a33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "total_tokens: 9"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "apollo_model.count_tokens(\"How many people are involved in this transcript?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfeAxehx79ng"
      },
      "source": [
        "## Delete the cache\n",
        "\n",
        "A cache object will be deleted automatically when it expires. You can also explicitly delete a cache object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HdP83dj08Nb1",
        "outputId": "b0f8a259-0dd3-46f6-e01e-51a677a7b596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cachedContents/c1tp9dntx6cs\n"
          ]
        }
      ],
      "source": [
        "print(apollo_cache.name)\n",
        "apollo_cache.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Caching.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}